{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOzTnyaPR5GkpwcKoUFDiNX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-oSVZCXZQg4D","executionInfo":{"status":"ok","timestamp":1712320893533,"user_tz":-240,"elapsed":1174,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"aa900b4b-cfc2-44d1-db50-38bb9af06231"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 's10-residual-ocp'...\n","remote: Enumerating objects: 70, done.\u001b[K\n","remote: Counting objects: 100% (70/70), done.\u001b[K\n","remote: Compressing objects: 100% (50/50), done.\u001b[K\n","remote: Total 70 (delta 30), reused 53 (delta 15), pack-reused 0\u001b[K\n","Receiving objects: 100% (70/70), 17.94 KiB | 2.99 MiB/s, done.\n","Resolving deltas: 100% (30/30), done.\n"]}],"source":["!git clone 'https://github.com/aakashvardhan/s10-residual-ocp.git'"]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/s10-residual-ocp')\n","sys.path.append('/content/s10-residual-ocp/models')"],"metadata":{"id":"tBiyA3QkRHtV","executionInfo":{"status":"ok","timestamp":1712320893534,"user_tz":-240,"elapsed":19,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["%cd /content/s10-residual-ocp\n","!pip install -r requirements.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRG3b-iERqSm","executionInfo":{"status":"ok","timestamp":1712321242374,"user_tz":-240,"elapsed":348858,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"6c1a81c3-bc4e-4023-9f13-1dce5ccf4526"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/s10-residual-ocp\n","Collecting torch_lr_finder (from -r requirements.txt (line 1))\n","  Downloading torch_lr_finder-0.2.1-py3-none-any.whl (11 kB)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from torch_lr_finder->-r requirements.txt (line 1)) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_lr_finder->-r requirements.txt (line 1)) (1.25.2)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from torch_lr_finder->-r requirements.txt (line 1)) (2.2.1+cu121)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_lr_finder->-r requirements.txt (line 1)) (4.66.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torch_lr_finder->-r requirements.txt (line 1)) (24.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (3.13.3)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (4.10.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (2023.6.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (2.2.0)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1))\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (4.50.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (1.4.5)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->torch_lr_finder->-r requirements.txt (line 1)) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=0.4.1->torch_lr_finder->-r requirements.txt (line 1)) (1.3.0)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch_lr_finder\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 torch_lr_finder-0.2.1\n"]}]},{"cell_type":"code","source":["from config import get_config\n","config = get_config()\n","from main import main\n","import torch\n","torch.manual_seed(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YJMUhUqfRQQQ","executionInfo":{"status":"ok","timestamp":1712321251574,"user_tz":-240,"elapsed":9205,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"51c58abd-d2c6-4415-fa86-978815bff376"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7c01d42e6850>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["config['epochs'] = 24\n","config['lr_scheduler'] = 'one_cycle'\n","config['lr'] = 0.05"],"metadata":{"id":"pACehiRcRTN_","executionInfo":{"status":"ok","timestamp":1712321394885,"user_tz":-240,"elapsed":2,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["model, test_loader, lr = main(config)"],"metadata":{"id":"nmTyjgulTvQR","executionInfo":{"status":"error","timestamp":1712321414155,"user_tz":-240,"elapsed":6467,"user":{"displayName":"Aakash Vardhan","userId":"03146811293329168388"}},"outputId":"52e53f62-121f-478d-cdfe-f97934fb1460","colab":{"base_uri":"https://localhost:8080/","height":1000}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA Available? True\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:01<00:00, 87683067.73it/s] \n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 64, 32, 32]           1,728\n","          Identity-2           [-1, 64, 32, 32]               0\n","          Identity-3           [-1, 64, 32, 32]               0\n","       BatchNorm2d-4           [-1, 64, 32, 32]             128\n","              ReLU-5           [-1, 64, 32, 32]               0\n","           Dropout-6           [-1, 64, 32, 32]               0\n","         ConvBlock-7           [-1, 64, 32, 32]               0\n","            Conv2d-8          [-1, 128, 32, 32]          73,728\n","         MaxPool2d-9          [-1, 128, 16, 16]               0\n","        MaxPool2d-10          [-1, 128, 16, 16]               0\n","      BatchNorm2d-11          [-1, 128, 16, 16]             256\n","             ReLU-12          [-1, 128, 16, 16]               0\n","          Dropout-13          [-1, 128, 16, 16]               0\n","        ConvBlock-14          [-1, 128, 16, 16]               0\n","           Conv2d-15          [-1, 128, 16, 16]         147,456\n","      BatchNorm2d-16          [-1, 128, 16, 16]             256\n","           Conv2d-17          [-1, 128, 16, 16]         147,456\n","      BatchNorm2d-18          [-1, 128, 16, 16]             256\n","         ResBlock-19          [-1, 128, 16, 16]               0\n","           Conv2d-20          [-1, 256, 16, 16]         294,912\n","        MaxPool2d-21            [-1, 256, 8, 8]               0\n","        MaxPool2d-22            [-1, 256, 8, 8]               0\n","      BatchNorm2d-23            [-1, 256, 8, 8]             512\n","             ReLU-24            [-1, 256, 8, 8]               0\n","          Dropout-25            [-1, 256, 8, 8]               0\n","        ConvBlock-26            [-1, 256, 8, 8]               0\n","           Conv2d-27            [-1, 512, 8, 8]       1,179,648\n","        MaxPool2d-28            [-1, 512, 4, 4]               0\n","        MaxPool2d-29            [-1, 512, 4, 4]               0\n","      BatchNorm2d-30            [-1, 512, 4, 4]           1,024\n","             ReLU-31            [-1, 512, 4, 4]               0\n","          Dropout-32            [-1, 512, 4, 4]               0\n","        ConvBlock-33            [-1, 512, 4, 4]               0\n","           Conv2d-34            [-1, 512, 4, 4]       2,359,296\n","      BatchNorm2d-35            [-1, 512, 4, 4]           1,024\n","           Conv2d-36            [-1, 512, 4, 4]       2,359,296\n","      BatchNorm2d-37            [-1, 512, 4, 4]           1,024\n","         ResBlock-38            [-1, 512, 4, 4]               0\n","        MaxPool2d-39            [-1, 512, 1, 1]               0\n","           Linear-40                   [-1, 10]           5,130\n","================================================================\n","Total params: 6,573,130\n","Trainable params: 6,573,130\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 9.44\n","Params size (MB): 25.07\n","Estimated Total Size (MB): 34.53\n","----------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n","  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Optimizer already has a scheduler attached to it","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-a5db96f8b983>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/s10-residual-ocp/main.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step_size\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mlr_plateau\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mlr_finder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLRFinder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_lr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"exp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_finder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# to inspect the loss-learning rate graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, optimizer, criterion, device, memory_cache, cache_dir)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Check if the optimizer is already attached to a scheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_for_scheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch_lr_finder/lr_finder.py\u001b[0m in \u001b[0;36m_check_for_scheduler\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam_group\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"initial_lr\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparam_group\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Optimizer already has a scheduler attached to it\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking_transfer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Optimizer already has a scheduler attached to it"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EtoR6x3cTydF"},"execution_count":null,"outputs":[]}]}